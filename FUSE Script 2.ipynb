{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script with ARD",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc8JiodOjZDg"
      },
      "source": [
        "1: To mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iY5BiiGGukB"
      },
      "source": [
        "%reset -f\n",
        "from google.colab import drive \n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvC3COILicVO"
      },
      "source": [
        "2: To install category_encoders - for pre-processing, not really needed as the code is commented out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdjL6I1RG2g5"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvrjgKLpIvNq"
      },
      "source": [
        "3: The function below is used to get the data from the files using the pandas library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIUj1H_w_9pz"
      },
      "source": [
        "import pandas as pd\n",
        "from category_encoders import OrdinalEncoder  # convert categorical features, if non it won't change the data\n",
        "\n",
        "def get_values_df(filename, CapType, print_data_bool):\n",
        "    df = pd.read_csv(filename, encoding= 'unicode_escape')\n",
        "    df.to_csv(filename, header=[\"Weight\", \"Thickness\", \"Porosity%\", CapType], index=False)\n",
        "    df.to_csv(filename, header=[\"Weight\", \"Thickness\", \"Porosity%\", CapType], index=False)\n",
        "\n",
        "    if print_data_bool == 'yes':\n",
        "        print(df)\n",
        "    return df"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RvoCrTmuCo"
      },
      "source": [
        "4: Splits df into X and Y "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AK81vTbmdOx"
      },
      "source": [
        "def get_values_XY(df, CapType):\n",
        "    df.dropna(subset = [CapType], inplace=True)\n",
        "    y_df=df[df.columns[column_of_prediction]].to_frame() # specify output\n",
        "    X_df=df[df.columns[0:column_of_prediction]] # anything except the output is regressor\n",
        "    categorical_features = [col for col in X_df.columns if X_df[col].dtype == 'object']\n",
        "    # encoder = OrdinalEncoder(cols=categorical_features).fit(X_df)\n",
        "    # X_df=encoder.transform(X_df)\n",
        "\n",
        "    return X_df, y_df"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYqfFXH2hraF"
      },
      "source": [
        "5: PCA below, first need to install PCA factor map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb7pWeW4blsl"
      },
      "source": [
        "pip install -i https://test.pypi.org/simple/ variable-factor-map-Huy-Bui==0.0.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSiAtosxAS8E"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from variable_factor_map import pca_map \n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "def PCA_function(X_df,show_bool, print_bool):\n",
        "    pca = PCA(n_components=2, svd_solver='full')\n",
        "    pca.fit(X_df)\n",
        "    pca_values = pca.components_\n",
        "    X_df_PCA = pca.transform(X_df)\n",
        "    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    px.area(\n",
        "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
        "    y=exp_var_cumul,\n",
        "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
        "    )\n",
        "\n",
        "    if print_bool == 'yes':\n",
        "        print('PCA explained variance ratio:', pca.explained_variance_ratio_)\n",
        "        print('PCA singular values:', pca.singular_values_)\n",
        "        print('PCA data shape: ', X_df_PCA.shape)\n",
        "        print('Initial data shape:', X_df.shape)\n",
        "        print('PCA components:', pca.components_, '\\n')\n",
        "        \n",
        "\n",
        "    if show_bool == 'yes':\n",
        "        pca_map(X_df, figsize=(10,10), sup='WMG Data', print_values= False)\n",
        "        plt.show()\n",
        "\n",
        "    return X_df_PCA, pca.explained_variance_ratio_, pca.singular_values_, pca.components_\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urP4AEmwhomI"
      },
      "source": [
        "6: Models and parameter grids below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG5kc4pI471f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import make_friedman2\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, ExpSineSquared\n",
        "from sklearn.linear_model import ARDRegression\n",
        "\n",
        "#Gradient Boosting\n",
        "GB = GradientBoostingRegressor()\n",
        "param_grid_GB = {'n_estimators': [10, 100, 500],\n",
        "              'learning_rate': [0.1, 0.05, 0.02],\n",
        "              'max_depth': [4],\n",
        "              'max_features': [\"log2\", \"sqrt\"],\n",
        "              'criterion': [\"friedman_mse\", \"mae\"],\n",
        "              'min_samples_split': [3],\n",
        "              'min_samples_leaf': [3],\n",
        "              'max_features': [1.0]}\n",
        "\n",
        "#SVR\n",
        "SVR_model = SVR(kernel = 'rbf')\n",
        "# param_grid_SVR_model={'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),\"C\": [1e0, 1e1, 1e2, 1e3, 1e4],\n",
        "#                 \"gamma\": np.logspace(-2, 3, 10),'degree' : [3,8]}\n",
        "param_grid_SVR_model={\"C\": [1e0, 1e1, 1e2, 1e3, 1e4],\n",
        "                \"gamma\": np.logspace(-2, 3, 10),'degree' : [3,8]}\n",
        "\n",
        "\n",
        "#MLP - NN\n",
        "MLP = MLPRegressor(alpha = 0.06, solver = \"lbfgs\", hidden_layer_sizes=3, learning_rate_init=0.1)\n",
        "param_grid_MLP = {'hidden_layer_sizes': [i for i in range(1,5)],\n",
        "                  'activation': [\"relu\"],\n",
        "                  'solver': [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "                  'learning_rate': ['constant'],\n",
        "                  'learning_rate_init': [i for i in [0.01,0.03,0.06,0.1]],\n",
        "                  'power_t': [0.5],\n",
        "                  'alpha': [i for i in [0.0001, 0.001,0.01]], #0.00001, 0.0001, 0.001, 0.01\n",
        "                  #0.0001 was used for C20\n",
        "                  'max_iter': [1000000],\n",
        "                  'early_stopping': [False],\n",
        "                  'warm_start': [False]}\n",
        "\n",
        "#Gaussian Process Regressor\n",
        "GPR = GaussianProcessRegressor()            \n",
        "param_grid_GPR = [{\"alpha\":  [1e0, 1e-1, 1e-2, 1e-3],\"kernel\": [RBF(l) for l in np.logspace(-1, 1, 2)]\n",
        "                   }, {\"alpha\":  [1e0, 1e-1, 1e-2, 1e-3],\"kernel\": [DotProduct(sigma_0) for sigma_0 in np.logspace(-1, 1, 2)]},\n",
        "                   {\"alpha\":  [1e0, 1e-1, 1e-2, 1e-3]}]\n",
        "# \"kernel\": [ExpSineSquared(l, p)+0.001*kernels.Identity()\n",
        "#                          for l in np.logspace(-2, 2, 10)\n",
        "#                          for p in np.logspace(0, 2, 10)]\n",
        "\n",
        "ARD = ARDRegression()\n",
        "param_grid_ARD = [{'alpha_1': [1e-1, 1e-2, 1e-3, 1e-5]}, {'alpha_2': [1e-1, 1e-2, 1e-3, 1e-5]}]\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euO3jC_KC7AQ"
      },
      "source": [
        "7. To save estimators, parameters and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAuJGDl1CWV9"
      },
      "source": [
        "import csv\n",
        "def save_estimators_parameters_results(AOrC, CapType, C_Rate, index_filename, model_name_list, index_model, PCA_bool, all_parameters, all_estimators, results):\n",
        "    with open(\n",
        "            r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_parameters_' +\n",
        "            C_Rate[\n",
        "                index_filename] + '_' + model_name_list[index_model] + '_' + PCA_bool + '.csv',\n",
        "            'w') as f:\n",
        "        for key in all_parameters.keys():\n",
        "            f.write(\"%s, %s\\n\" % (key, all_parameters[key]))\n",
        "    with open(\n",
        "            r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_estimators_' +\n",
        "            C_Rate[\n",
        "                index_filename] + '_' + model_name_list[index_model] + '_' + PCA_bool + '.csv',\n",
        "            'w') as f:\n",
        "        for key in all_parameters.keys():\n",
        "            f.write(\"%s, %s\\n\" % (key, all_estimators[key]))\n",
        "    \n",
        "    np.savetxt(\n",
        "        r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_results_' +\n",
        "        C_Rate[index_filename] + '_' + model_name_list[index_model] + '_' + PCA_bool + '.csv',\n",
        "        results,\n",
        "        delimiter=\",\")  # uncomment these two lines if the results for the individual C_rates are needed in individual files"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDEPtYJeZJs7"
      },
      "source": [
        "8. Functions for feature importance for automatic relevance determination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFrUBhV3WuHG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def get_plots_ARD_coefs(estimator_, n_features):   \n",
        "      relevant_features = np.arange(0,n_features)\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      plt.title(\"Weights of the model\")\n",
        "      plt.plot(estimator_.coef_, color='darkblue', linestyle='-', linewidth=2,\n",
        "         label=\"ARD estimate\")\n",
        "      plt.xlabel(\"Features\")\n",
        "      plt.ylabel(\"Values of the weights\")\n",
        "      plt.legend(loc=1)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      plt.title(\"Histogram of the weights\")\n",
        "      plt.hist(estimator_.coef_, bins=n_features, color='navy', log=True)\n",
        "      plt.scatter(estimator_.coef_[relevant_features], np.full(len(relevant_features), 5.),\n",
        "            color='gold', marker='o', label=\"Relevant features\")\n",
        "      plt.ylabel(\"Features\")\n",
        "      plt.xlabel(\"Values of the weights\")\n",
        "      plt.legend(loc=1)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(6, 5))\n",
        "      plt.title(\"Marginal log-likelihood\")\n",
        "      plt.plot(estimator_.scores_, color='navy', linewidth=2)\n",
        "      plt.ylabel(\"Score\")\n",
        "      plt.xlabel(\"Iterations\") \n",
        "      plt.show()    \n",
        "\n",
        "def get_FI(regr, X_df, FI_plot_bool):    \n",
        "    if model == GB: \n",
        "      importance = regr.best_estimator_.feature_importances_\n",
        "      if print_bool == 'yes':\n",
        "        for i,v in enumerate(importance):\n",
        "          print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "      \n",
        "      plt.bar([x for x in range(len(importance))], importance)\n",
        "      plt.xlabel(\"Weight, Thickness, Porosity%\")\n",
        "      plt.ylabel(\"Feature Importance\")\n",
        "      if FI_plot_bool == 'yes':  \n",
        "        pyplot.show()\n",
        "    if model == SVR_model:\n",
        "      importance = regr.best_estimator_.dual_coef_\n",
        "    if model == GPR:\n",
        "      importance = regr.best_estimator_.alpha_\n",
        "    if model == ARD:\n",
        "      importance = regr.best_estimator_.coef_ \n",
        "      n_features = X_df.shape[1]\n",
        "      if FI_plot_bool == 'yes':\n",
        "          get_plots_ARD_coefs(regr.best_estimator_, n_features)\n",
        "    \n",
        "    if print_bool == 'yes':\n",
        "        print('Feature importance: \\n', importance)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpRNhJshkkm"
      },
      "source": [
        "9. Functions used for training  below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQfLHdoQhef3"
      },
      "source": [
        "from IPython.display import display_html\n",
        "\n",
        "# apply the maximum absolute scaling in Pandas using the .abs() and .max() methods\n",
        "def maximum_absolute_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_scaled = df.copy()\n",
        "    # apply maximum absolute scaling\n",
        "    for column in df_scaled.columns:\n",
        "        df_scaled[column] = df_scaled[column] / df_scaled[column].abs().max()\n",
        "    return df_scaled\n",
        "\n",
        "def display_side_by_side(*args):\n",
        "    html_str=''\n",
        "    for df in args:\n",
        "        html_str+=df.to_html()\n",
        "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
        "\n",
        "def train(X_df, y_df, model, param_grid_in_use, print_bool, FI_bool, FI_plot_bool):\n",
        "    \n",
        "    _GS = GridSearchCV(model, param_grid=param_grid_in_use, n_jobs = -1)\n",
        "\n",
        "    X, y = make_regression(n_samples=200, random_state=0)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, random_state=0)\n",
        "    regr = _GS.fit(X_train, y_train.values.ravel())\n",
        "    # regr = model.fit(X_train, y_train.values.ravel()) # use this when not using gridsearch\n",
        "\n",
        "    # regr.predict(New_data) #for prediction on data outside dataset\n",
        "    \n",
        "    # y_pred = pd.DataFrame(regr.predict(X_test), columns=['pred'], index=X_test.index) # use this when not using gridsearch\n",
        "    y_pred = pd.DataFrame(regr.predict(X_test), columns=['pred'])\n",
        "\n",
        "\n",
        "    # comment out to make reading easier\n",
        "    # display_side_by_side(y_pred, y_test)\n",
        "    error = y_pred.values-y_test.values\n",
        "    if print_bool == 'yes':\n",
        "      print('regr score is', regr.score(X_test, y_test))\n",
        "      print('RMSE', np.mean(np.absolute(error)))\n",
        "\n",
        "    if FI_bool == 'yes':\n",
        "      get_FI(regr, X_df, FI_plot_bool)\n",
        "\n",
        "    return regr.score(X_test, y_test), np.mean(np.absolute(error)), regr.best_score_, regr.best_estimator_, regr.best_params_\n",
        "    # return regr.score(X_test, y_test), np.mean(np.absolute(error)) #use this when not using grid search\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i50kQ_f-yPTv"
      },
      "source": [
        "10. The function below is to plot all the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxA9FY7ZuO5A"
      },
      "source": [
        "from matplotlib.patches import Patch\n",
        "\n",
        "def get_plots(AOrC, CapType, result_name, C_Rate, model_name_list, PCA_bool, analytics, stds, save_plot_bool):\n",
        "    color_list = ['red', 'green', 'blue', 'black']\n",
        "    x_pos = np.arange(len(result_name))\n",
        "    # Build the plot\n",
        "    fig, axs = plt.subplots(len(C_Rate))\n",
        "    # fig.suptitle('All C Rates Results', fontsize = 20)\n",
        "    fig.subplots_adjust(top=0.8, hspace=0.5)\n",
        "    for i in range(len(C_Rate)):\n",
        "        for j in range(len(model_name_list)):\n",
        "            axs[i].bar(x_pos*len(model_name_list)+j, analytics[i,j], yerr=stds[i,j], align='center', alpha=1/len(model_name_list), ecolor='black', color =color_list[j] , capsize=10, label = C_Rate[i])\n",
        "        axs[i].set_ylabel('Results', fontsize = 5)\n",
        "        axs[i].set_xticks(x_pos, result_name)\n",
        "        axs[i].set_xlabel(', '.join(map(str, result_name)), fontsize=10)\n",
        "        axs[i].set_title('Results of models on I_O_' + AOrC + '_' + CapType + '_plots_' + PCA_bool +'_'+ C_Rate[i], fontsize = 15)\n",
        "        axs[i].yaxis.grid(True)\n",
        "\n",
        "    plt.subplots_adjust(left=0.1,\n",
        "                    bottom=1.0, \n",
        "                    right=0.9, \n",
        "                    top=7.0, \n",
        "                    wspace=0.4, \n",
        "                    hspace=0.4)\n",
        "\n",
        "    # map names to colors\n",
        "    cmap = dict(zip(model_name_list, color_list))\n",
        "    # create the rectangles for the legend\n",
        "    patches = [Patch(color=v, label=k) for k, v in cmap.items()]\n",
        "    # add the legend\n",
        "    plt.legend(title='Type of Model', labels=model_name_list, handles=patches, bbox_to_anchor=(1.04, 0.5), loc='center left', borderaxespad=0, fontsize=15, frameon=False)\n",
        "\n",
        "    # Save the figure and show\n",
        "    plt.tight_layout(pad=100.0) \n",
        "    plt.show()\n",
        "    if save_plot_bool == 'yes':\n",
        "        plt.savefig(r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_' + AOrC + '_' + CapType + '_plots_' + '_' + model_name_list[index_model] + '_'+ PCA_bool +'_'.join(result_name) + '_'+ PCA_bool + '.png', dpi=600)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm5R6xZesoL-"
      },
      "source": [
        "11: Initialise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbRQ2QCaodIW"
      },
      "source": [
        "# filename = r'/gdrive/My Drive/FUSE data/Experimental-Dataset.csv'\n",
        "C_Rate = [\"C20\", \"C5\", \"C2\", \"1C\", \"2C\", \"5C\", \"10C\"]\n",
        "CapType_list = [\"Cap\", \"VCap\", \"GCap\"]\n",
        "PCA_bool_list = [\"non-PCA\", \"PCA\"]\n",
        "AOrC_list = [\"Anode\", \"Cathode\"]\n",
        "column_of_prediction = 3\n",
        "# filename = r'/gdrive/My Drive/FUSE data/I_O_'+AOrC+'_'+CapType+'/I_O_'+AOrC+'_'+CapType+'_'+C_Rate[index_filename]+'.csv'\n",
        "\n",
        "#options for model are GB, SVR_model, GPR, MLP\n",
        "model_list = [ARD, GB, SVR_model, GPR]\n",
        "model_name_list = ['ARD', 'GB', 'SVR_model', 'GPR']\n",
        "param_grid_list = [param_grid_ARD, param_grid_GB, param_grid_SVR_model, param_grid_GPR]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAP2vVnYrIRB"
      },
      "source": [
        "12: Add header to file names - this has to be run twice due to df.to_csv function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76O0HY_QqxWI"
      },
      "source": [
        "for index_AOrC in range(len(AOrC_list)):\n",
        "    for index_CapType in range(len(CapType_list)):\n",
        "            for index_filename in range(len(C_Rate)):\n",
        "                print_data_bool = 'no' #print data bool is set to no \n",
        "                filename = r'/gdrive/My Drive/FUSE data/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'_'+C_Rate[index_filename]+'.csv'\n",
        "                # print(filename)\n",
        "                #get values\n",
        "                df = get_values_df(filename, CapType_list[index_CapType], print_data_bool)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4uaSpNlsgMO"
      },
      "source": [
        "for index_AOrC in range(len(AOrC_list)):\n",
        "    for index_CapType in range(len(CapType_list)):\n",
        "            for index_filename in range(len(C_Rate)):\n",
        "                print_data_bool = 'no' #print data bool is set to no \n",
        "                filename = r'/gdrive/My Drive/FUSE data/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'/I_O_'+AOrC_list[index_AOrC]+'_'+CapType_list[index_CapType]+'_'+C_Rate[index_filename]+'.csv'\n",
        "                # print(filename)\n",
        "                #get values\n",
        "                df = get_values_df(filename, CapType_list[index_CapType], print_data_bool)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6USKgfhhiRP"
      },
      "source": [
        "13: Main run is below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQAyzeqhjd93"
      },
      "source": [
        "import time\n",
        "import csv\n",
        "import statistics as st\n",
        "import numpy as np\n",
        "for index_AOrC in range(len(AOrC_list)):\n",
        "    for index_PCA_bool in range(len(PCA_bool_list)):\n",
        "        for index_CapType in range(len(CapType_list)):\n",
        "            compiled_results = []\n",
        "            result_name = ['R^2', 'RMSE', 'Time']\n",
        "            analytics = np.zeros((len(C_Rate), len(model_name_list), len(result_name)))\n",
        "            stds = np.zeros((len(C_Rate), len(model_name_list), len(result_name)))\n",
        "            for index_filename in range(len(C_Rate)):\n",
        "                for index_model in range(len(model_name_list)):\n",
        "\n",
        "                    # option to print everything out\n",
        "                    print_bool = 'no'\n",
        "                    # option to print data\n",
        "                    print_data_bool = 'no'\n",
        "\n",
        "                    # option to use PCA or non-PCA data\n",
        "                    PCA_bool = PCA_bool_list[index_PCA_bool]\n",
        "                    # option to show the PCA map\n",
        "                    show_PCA_map_bool = 'no'\n",
        "                    # option to scale features\n",
        "                    scale_bool = 'no'\n",
        "                    \n",
        "                    # option to save the final results\n",
        "                    save_estimators_parameters_results_bool = 'no'\n",
        "                    # option to save the final results all compiled into one file\n",
        "                    save_compiled_results_bool = 'yes'\n",
        "                    # option to save the final results plot - this option has issues, need to manually save plots\n",
        "                    save_plot_bool = 'no'\n",
        "                    \n",
        "                    # option to print feature importance plot\n",
        "                    FI_bool = 'no'\n",
        "                    # option to show FI plots\n",
        "                    FI_plot_bool = 'no'\n",
        "\n",
        "                    # initialise lists and choose model\n",
        "                    scores = []\n",
        "                    RMSEs = []\n",
        "                    all_parameters = {}\n",
        "                    all_estimators = {}\n",
        "                    times = []\n",
        "                    model = model_list[index_model]\n",
        "                    model_name = model_name_list[index_model]\n",
        "                    param_grid_in_use = param_grid_list[index_model]\n",
        "\n",
        "                    #printing filename to check progress\n",
        "                    print('\\nI_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '_' + C_Rate[index_filename] + '_' + model_name_list[\n",
        "                        index_model] + '_' + PCA_bool)\n",
        "                    filename = r'/gdrive/My Drive/FUSE data/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '_' + \\\n",
        "                               C_Rate[index_filename] + '.csv'\n",
        "                    # print(filename)\n",
        "\n",
        "                    # get values\n",
        "                    df = get_values_df(filename, CapType_list[index_CapType], print_data_bool)\n",
        "                    X_df, y_df = get_values_XY(df, CapType_list[index_CapType])\n",
        "\n",
        "                    # get only numerical values and call the maximum_absolute_scaling function, currently not used\n",
        "                    if scale_bool == 'yes':\n",
        "                        X_df = maximum_absolute_scaling(X_df)\n",
        "\n",
        "                    # run PCA\n",
        "                    X_df_PCA, PCA_explained_variance_ratio_, PCA_singular_values_, PCA_components_ = PCA_function(X_df, show_PCA_map_bool, print_bool)\n",
        "\n",
        "                    # to print parameters and group them in an array to print the learning curve later\n",
        "                    epochs = 3\n",
        "                    for i in range(epochs):\n",
        "\n",
        "                        print('Run:', i + 1)\n",
        "                        #To assign PCA or non-PCA X to train\n",
        "                        if PCA_bool == 'PCA':\n",
        "                            X_df_dummy = X_df_PCA\n",
        "                        else:\n",
        "                            X_df_dummy = X_df\n",
        "\n",
        "                        #Start training    \n",
        "                        start = time.time()\n",
        "                        score, error, best_score, best_estimator, best_params = train(X_df_dummy, y_df, model, param_grid_in_use, print_bool, FI_bool, FI_plot_bool)\n",
        "                        # score, error = train(X_df_PCA, y_df) #for use when not using grid search\n",
        "                        stop = time.time()\n",
        "                        print(f\"Training time: {stop - start}s\")\n",
        "                        times.append(stop - start)\n",
        "\n",
        "                        # storing results from multiple runs\n",
        "                        scores.append(score) #R^2 scores\n",
        "                        RMSEs.append(error) #RMSE scores\n",
        "                        all_estimators[str(i)] = best_estimator  # comment out when not using grid search\n",
        "                        all_parameters[str(i)] = best_params  # comment out when not using grid search\n",
        "\n",
        "                    # storing mean and stdev of results\n",
        "                    compiled_results.append([st.mean(scores), st.stdev(scores), st.mean(RMSEs), st.stdev(RMSEs), st.mean(times),st.stdev(times)]) #for the final table\n",
        "                    results = np.asarray([scores, RMSEs, times])  # individual file results\n",
        "                    analytics[index_filename, index_model] = [st.mean(scores), st.mean(RMSEs), st.mean(times)] #to store all mean values in array\n",
        "                    stds[index_filename, index_model] = [st.stdev(scores), st.stdev(RMSEs), st.stdev(times)] #to store all std values in array\n",
        "                    if print_bool == 'yes':\n",
        "                        print(results)\n",
        "\n",
        "                    #to write the data R^2, RMSE and best estimators, parameters and results parameters to .csv files, only change save_estimators&parameters&results_bool to 'yes' if there is space to store everything\n",
        "                    if save_estimators_parameters_results_bool == 'yes':\n",
        "                        save_estimators_parameters_results(AOrC_list[index_AOrC], CapType_list[index_CapType], C_Rate, index_filename, model_name_list, index_model, PCA_bool, all_parameters, all_estimators, results)\n",
        "                        print('Saved estimators, parameters and results for '+model_name_list[index_model]+' '+PCA_bool+' run on '+filename)\n",
        "\n",
        "            # the time plot is separated from the other result metrics\n",
        "            get_plots(AOrC_list[index_AOrC], CapType_list[index_CapType], result_name[0:-1], C_Rate, model_name_list, PCA_bool, analytics[:, :, 0:-1], stds[:, :, 0:-1],\n",
        "                      save_plot_bool)\n",
        "            get_plots(AOrC_list[index_AOrC], CapType_list[index_CapType], [result_name[-1]], C_Rate, model_name_list, PCA_bool, analytics[:, :, -1], stds[:, :, -1],\n",
        "                      save_plot_bool)\n",
        "\n",
        "            #save compiled results\n",
        "            compiled_results_array = np.asarray(compiled_results)\n",
        "            if save_compiled_results_bool == 'yes':\n",
        "                np.savetxt(\n",
        "                    r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '/results/I_O_' + AOrC_list[index_AOrC] + '_' + CapType_list[index_CapType] + '_Compiled_Results_' + PCA_bool + '.csv', compiled_results_array, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq5fCVpj_lgP"
      },
      "source": [
        "This final section allows you to get the plots again. It make it easier to change the plot settings and not needing to run the training section again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw9Utn0fdmMe"
      },
      "source": [
        "save_plot_bool = 'no'\n",
        "get_plots(result_name[0:-1], C_Rate, model_name_list, PCA_bool, analytics[:,:,0:-1], stds[:,:,0:-1], save_plot_bool)\n",
        "get_plots([result_name[-1]], C_Rate, model_name_list, PCA_bool, analytics[:,:,-1], stds[:,:,-1], save_plot_bool)\n",
        "\n",
        "save_compiled_results_bool = 'no'\n",
        "if save_compiled_results_bool == 'yes':\n",
        "    np.savetxt(r'/gdrive/MyDrive/FUSE data/I_O_' + AOrC + '_' + CapType + '/results/I_O_'+ AOrC + '_' + CapType + '_Compiled_Results_' + PCA_bool + '.csv', compiled_results_array, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}